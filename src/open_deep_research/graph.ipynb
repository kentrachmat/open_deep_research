{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Bibliography generation\n",
    "(https://github.com/langchain-ai/open_deep_research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_API = \"arxiv\" # tavily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U -q open-deep-research\n",
    "# ! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load .env for all keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.10\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import open_deep_research   \n",
    "print(open_deep_research.__version__) \n",
    "\n",
    "import os\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langgraph.types import Command\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from open_deep_research.graph import builder\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract paper content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"LLM Evaluators Recognize and Favor Their OwnGenerations\"\n",
    "abstract = \"Self-evaluation using large language models (LLMs) has proven valuable not onlyin benchmarking but also methods like reward modeling, constitutional AI, andself-refinement.  But new biases are introduced due to the same LLM acting asboth the evaluator and the evaluatee. One such bias is self-preference, where anLLM evaluator scores its own outputs higher than others while human annotatorsconsider them of equal quality. But do LLMs actually recognize their own outputswhen they give those texts higher scores, or is it just a coincidence? In this paper, weinvestigate if self-recognition capability contributes to self-preference. We discoverthat, out of the box, LLMs such as GPT-4 and Llama 2 have non-trivial accuracy atdistinguishing themselves from other LLMs and humans. By fine-tuning LLMs, wediscover a linear correlation between self-recognition capability and the strengthof self-preference bias; using controlled experiments, we show that the causalexplanation resists straightforward confounders. We discuss how self-recognitioncan interfere with unbiased evaluations and AI safety more generally.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing '../../data/14702_LLM_Evaluators_Recognize.pdf'...\n",
      "Finished processing '../../data/14702_LLM_Evaluators_Recognize.pdf'.\n"
     ]
    }
   ],
   "source": [
    "# Input the paper as topic\n",
    "import fitz  # PyMuPDF library is imported as 'fitz'\n",
    "import sys\n",
    "\n",
    "def extract_text_from_pdf_pymupdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from all pages of a PDF file using PyMuPDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The file path to the PDF.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted text from the PDF, with pages separated by newlines.\n",
    "             Returns None if the file cannot be processed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at '{pdf_path}'\", file=sys.stderr)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening or processing PDF '{pdf_path}': {e}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "    all_text = \"\"\n",
    "    print(f\"Processing '{pdf_path}'...\")\n",
    "    with doc:  # Ensures the document is closed properly\n",
    "        for page_num, page in enumerate(doc):\n",
    "            try:\n",
    "                page_text = page.get_text(\"text\") # Extract text as plain text\n",
    "                if page_text: # Only add if text was found\n",
    "                   all_text += f\"--- Page {page_num + 1} ---\\n\"\n",
    "                   all_text += page_text.strip() + \"\\n\\n\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting text from page {page_num + 1}: {e}\", file=sys.stderr)\n",
    "                # Optionally continue to the next page or stop\n",
    "                # continue\n",
    "\n",
    "    if not all_text:\n",
    "         print(\"Warning: No text could be extracted. The PDF might be image-based (scanned).\", file=sys.stderr)\n",
    "\n",
    "    print(f\"Finished processing '{pdf_path}'.\")\n",
    "    return all_text.strip() # Remove trailing newlines\n",
    "\n",
    "# --- Example Usage ---\n",
    "pdf_file = \"../../data/14702_LLM_Evaluators_Recognize.pdf\"\n",
    "\n",
    "full_paper_text = extract_text_from_pdf_pymupdf(pdf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start generating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Please provide feedback on the following bibliography plan. \n",
       "                        \n",
       "\n",
       "Section: Abstract\n",
       "Description: An overview of the paper's focus on self-recognition and self-preference in LLM evaluation, summarizing methodology and key findings.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Introduction\n",
       "Description: This section covers the background, motivation, and the stated research questions regarding how LLM evaluators tend to favor their own outputs.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Definition and Measurement of Self-Preference and Self-Recognition\n",
       "Description: Discusses the conceptual definitions and detailed experimental designs for measuring self-preference bias and self-recognition capability in LLMs.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Measuring Correlation between Self-Preference and Self-Recognition\n",
       "Description: Focuses on the methodologies and fine-tuning experiments that link self-recognition capability to self-preference bias, including controlled experiments and statistical evaluations.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Related Work\n",
       "Description: Summarizes previous literature on LLM self-evaluation, bias in model assessments, and methods in LLM detection and introspection, relating them to the user-provided papers contributions.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Limitations, Discussion, and Conclusion\n",
       "Description: Covers the discussion of safety implications, limitations of the current study, future research directions, and a concluding synthesis of the insights gained.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "                        \n",
       "Does the bibliography plan meet your needs?\n",
       "Pass 'true' to approve the bibliography plan.\n",
       "Or, provide feedback to regenerate the bibliography plan:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import uuid \n",
    "from IPython.display import Markdown\n",
    "\n",
    "REPORT_STRUCTURE = \"\"\"Use this structure to create a bibliography on the user-provided paper:\n",
    "\n",
    "1. Abstract\n",
    "   - Brief overview of the topic area\n",
    "\n",
    "2. Main Body Sections:\n",
    "   - Each section's title is the same as each user-provided paper's section title.\n",
    "   \n",
    "3. Conclusion\n",
    "   - Aim for 1 structural element (either a list of table) that distills the main body sections \n",
    "   - Provide a concise summary of the bibliography\"\"\"\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": str(uuid.uuid4()),\n",
    "                           \"search_api\": SEARCH_API,\n",
    "                           \"planner_provider\": os.environ[\"MODEL_PROVIDER\"],\n",
    "                           \"planner_model\": os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "                           \"writer_provider\": os.environ[\"MODEL_PROVIDER\"],\n",
    "                           \"writer_model\": os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"], \n",
    "                           \"report_structure\": REPORT_STRUCTURE,\n",
    "                           \"max_search_depth\": 1,}\n",
    "                           } \n",
    "\n",
    "topic = \"Bibliograp hy of the paper : \" + title\n",
    "\n",
    "# Run the graph until the interruption\n",
    "async for event in graph.astream({\"paper_content\":full_paper_text,\n",
    "                                  \"title\": title,\n",
    "                                  \"abstract\": abstract,\n",
    "                                  \"topic\": topic\n",
    "                                  }, thread, stream_mode=\"updates\"):\n",
    "\n",
    "   if '__interrupt__' in event:\n",
    "        interrupt_value = event['__interrupt__'][0].value\n",
    "        display(Markdown(interrupt_value))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass feedback to update the report plan  (skipped for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pass feedback to update the report plan  (skipped for now)\n",
    "# feedback = \"That's a good plan.\"\n",
    "# async for event in graph.astream(Command(resume=feedback), thread, stream_mode=\"updates\"):\n",
    "#     if '__interrupt__' in event:\n",
    "#         interrupt_value = event['__interrupt__'][0].value\n",
    "#         display(Markdown(interrupt_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'human_feedback': None}\n",
      "\n",
      "\n",
      "Error processing arXiv query 'LLM evaluators bias self-generated content favoring own outputs': module 'fitz' has no attribute 'fitz'\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Introduction', description='This section covers the background, motivation, and the stated research questions regarding how LLM evaluators tend to favor their own outputs.', research=True, content='## Introduction\\n\\nLLM evaluators play a crucial role in assessing language generation quality. Recent studies reveal that both user interactions and automated evaluations may be subject to inherent biases. For instance, research indicates users often prefer responses where inaccuracies are unmarked, even when such responses are confidently delivered [1]. This suggests that evaluative systems might similarly be predisposed, potentially leading to an inadvertent favoring of self-generated outputs.\\n\\nMotivated by these observations, recent work has started to explore how fairness in predictive preferences can improve the alignment of automated judgments with human expectations [2]. Such findings prompt questions regarding whether LLM evaluators recognize and favor their own generations, even when alternative responses might be more accurate. Addressing this research question is critical for improving the reliability and trustworthiness of LLM outputs. This investigation underscores the need for methods that mitigate evaluative bias to achieve a more balanced and fair assessment in language technology.\\n\\n### Sources\\n[1] Fool Me, Fool Me: User Attitudes Toward LLM Falsehoods: http://arxiv.org/abs/2412.11625v1  \\n[2] Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments: http://arxiv.org/abs/2406.11370v2')]}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Definition and Measurement of Self-Preference and Self-Recognition', description='Discusses the conceptual definitions and detailed experimental designs for measuring self-preference bias and self-recognition capability in LLMs.', research=True, content='## Definition and Measurement of Self-Preference and Self-Recognition\\n\\nSelf-preference bias refers to the tendency of a language model to favor its own outputs over those from other sources. Researchers define this bias by comparing evaluation scores assigned to self-generated texts versus externally produced texts. A key aspect of measuring the bias is the use of perplexity, where lower perplexity suggests greater text familiarity and leads to higher self-evaluation [1].\\n\\nExperimental designs for assessing self-recognition involve controlled tests where models evaluate both self-generated and externally sourced responses. The novel quantitative metric examines differences in perplexity and assessment scores, thereby highlighting the role of self-recognition in driving self-preference. In addition, researchers have begun to investigate self-recognition by integrating diverse evaluation frameworks and employing both intrinsic and extrinsic metrics. This multifaceted approach aids in isolating the effect of familiarity and response style on bias, contributing to improved fairness and reliability in automated evaluations.\\n\\n### Sources\\n[1] Self-Preference Bias in LLM-as-a-Judge: http://arxiv.org/abs/2410.21819v1')]}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Related Work', description='Summarizes previous literature on LLM self-evaluation, bias in model assessments, and methods in LLM detection and introspection, relating them to the user-provided papers contributions.', research=True, content=\"## Related Work\\nPrevious research has explored whether large language models (LLMs) can reliably introspect their own internal states. For example, Song et al. [1] systematically investigated metalinguistic prompting to compare models self-reported probabilities with their internal string probability estimates, finding limited evidence for privileged self-access. In another vein, studies in robotics have leveraged introspective planning techniques to align LLMs uncertainty with task ambiguity, improving both safety and compliance [2]. Other works introduced introspective tips that allow LLMs to refine decision-making by reflecting on past interactions without parameter fine-tuning [3]. In parallel, bias evaluation in language models has attracted significant attention. Researchers have measured gender bias using both lexicon-based and novel model-based approaches, revealing instability in results dependent on metrics and data dependencies [4]. Additional tools, such as BiasAlert, have been proposed to detect social bias in open-text outputs, providing flexible alternatives to fixed-form assessments [5]. Furthermore, a new benchmark comparing multiple bias evaluation methods in generation tasks highlights inconsistencies between long-form and multiple-choice evaluations [6]. These works lay the groundwork for our study on LLM evaluators favoring their own generations.\\n\\n### Sources\\n[1] Language Models Fail to Introspect About Their Knowledge of Language: http://arxiv.org/abs/2503.07513v2  \\n[2] Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity: http://arxiv.org/abs/2402.06529v4  \\n[3] Introspective Tips: Large Language Model for In-Context Decision Making: http://arxiv.org/abs/2305.11598v1  \\n[4] What is Your Favorite Gender, MLM? Gender Bias Evaluation in Multilingual Masked Language Models: http://arxiv.org/abs/2404.06621v1  \\n[5] BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs: http://arxiv.org/abs/2407.10241v2  \\n[6] Social Bias Benchmark for Generation: A Comparison of Generation and QA-Based Evaluations: http://arxiv.org/abs/2503.06987v1\")]}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Abstract', description=\"An overview of the paper's focus on self-recognition and self-preference in LLM evaluation, summarizing methodology and key findings.\", research=True, content='## Abstract\\n\\nThis paper examines a unique phenomenon where LLM evaluators exhibit self-recognition and self-preference when judging their own generations. Our work investigates how evaluation systems based on natural language responses may inherently favor outputs originating from the same model. The methodology combines error-based human annotation with meta-evaluation techniques to analyze the extent of evaluation biases. The findings indicate that self-generated outputs are consistently rated higher than independent ones, suggesting a systematic bias in automated evaluation protocols. This study builds on existing frameworks, such as the comprehensive survey of LLMs-as-judges [1] and hierarchical evaluation methods proposed in TencentLLMEval [2]. These sources provide essential insights into current evaluation methodologies and underscore challenges in ensuring objective assessments. Our results advocate for enhanced evaluation protocols that mitigate self-preference, thereby promoting fairness and improving reliability in LLM performance assessments across diverse tasks.\\n\\n### Sources\\n[1] LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods: http://arxiv.org/abs/2412.05579v2  \\n[2] TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for Human-Aligned LLMs: http://arxiv.org/abs/2311.05374v1')]}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Measuring Correlation between Self-Preference and Self-Recognition', description='Focuses on the methodologies and fine-tuning experiments that link self-recognition capability to self-preference bias, including controlled experiments and statistical evaluations.', research=True, content='## Measuring Correlation between Self-Preference and Self-Recognition\\n\\nResearch in this area focuses on evaluating how large language models (LLMs) develop self-preference biases through their inherent self-recognition abilities. Panickssery et al. [1] provide experimental evidence that LLMs like GPT-4 and Llama 2 can distinguish their own outputs from those generated by others. Their controlled fine-tuning experiments reveal a linear relationship between the self-recognition score and the strength of self-preference bias, using statistical analyses to rule out straightforward confounders.\\n\\nChen et al. [2] further examine self-preference by applying objective benchmarks in evaluating whether higher scores correspond to genuinely superior outputs or an inherent favoritism. Their methodologies employ verifiable tasks, such as mathematical reasoning and code generation, to differentiate beneficial self-recognition from harmful bias. Collectively, these studies emphasize the importance of controlled evaluation and fine-tuning experiments in understanding the causal link between a models self-recognition capability and its tendency to favor its own generations, contributing to improved reliability and AI safety.\\n\\n### Sources\\n[1] LLM Evaluators Recognize and Favor Their Own Generations: http://arxiv.org/abs/2404.13076v1  \\n[2] Do LLM Evaluators Prefer Themselves for a Reason?: http://arxiv.org/abs/2504.03846v1')]}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Limitations, Discussion, and Conclusion', description='Covers the discussion of safety implications, limitations of the current study, future research directions, and a concluding synthesis of the insights gained.', research=True, content='## Limitations, Discussion, and Conclusion\\n\\nThis research section emphasizes several limitations and safety implications of using LLM evaluators. A key finding is the self-preference bias, where models assign higher scores to their own outputs, potentially misrepresenting actual quality and harming fair comparisons [1]. The study also notes inherent limitations in LLM performances, including reasoning flaws, memory issues, and calculation errors that restrict their reliability for critical applications [2].\\n\\nMoreover, our discussion indicates that current evaluation frameworks may fall short in capturing complex real-world interactions. Incorporating human interaction evaluations can strengthen safety assessments and mitigate risks associated with biased self-recognition [3]. In conclusion, while LLM evaluators bring promising advances to automated assessments, their biases and operational limitations call for refined methods and integrated human oversight. Future research should focus on developing robust testing frameworks that blend technical improvements with comprehensive safety and governance strategies.\\n\\n### Sources\\n[1] LLM Evaluators Recognize and Favor Their Own Generations: http://arxiv.org/abs/2404.13076v1  \\n[2] Responsible AI in Construction Safety: Systematic Evaluation of Large Language Models and Prompt Engineering: http://arxiv.org/abs/2411.08320v1  \\n[3] Beyond static AI evaluations: advancing human interaction evaluations for LLM harms and risks: http://arxiv.org/abs/2405.10632v5')]}}\n",
      "\n",
      "\n",
      "sources=[Source(name='LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods', link='http://arxiv.org/abs/2412.05579v2'), Source(name='TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for Human-Aligned LLMs', link='http://arxiv.org/abs/2311.05374v1'), Source(name='Fool Me, Fool Me: User Attitudes Toward LLM Falsehoods', link='http://arxiv.org/abs/2412.11625v1'), Source(name='Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments', link='http://arxiv.org/abs/2406.11370v2'), Source(name='Self-Preference Bias in LLM-as-a-Judge', link='http://arxiv.org/abs/2410.21819v1'), Source(name='LLM Evaluators Recognize and Favor Their Own Generations', link='http://arxiv.org/abs/2404.13076v1'), Source(name='Do LLM Evaluators Prefer Themselves for a Reason?', link='http://arxiv.org/abs/2504.03846v1'), Source(name='Language Models Fail to Introspect About Their Knowledge of Language', link='http://arxiv.org/abs/2503.07513v2'), Source(name=\"Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity\", link='http://arxiv.org/abs/2402.06529v4'), Source(name='Introspective Tips: Large Language Model for In-Context Decision Making', link='http://arxiv.org/abs/2305.11598v1'), Source(name='What is Your Favorite Gender, MLM? Gender Bias Evaluation in Multilingual Masked Language Models', link='http://arxiv.org/abs/2404.06621v1'), Source(name='BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs', link='http://arxiv.org/abs/2407.10241v2'), Source(name='Social Bias Benchmark for Generation: A Comparison of Generation and QA-Based Evaluations', link='http://arxiv.org/abs/2503.06987v1'), Source(name='Responsible AI in Construction Safety: Systematic Evaluation of Large Language Models and Prompt Engineering', link='http://arxiv.org/abs/2411.08320v1'), Source(name='Beyond static AI evaluations: advancing human interaction evaluations for LLM harms and risks', link='http://arxiv.org/abs/2405.10632v5')]\n",
      "{'gather_completed_sections': {'report_sections_from_research': \"\\n============================================================\\nSection 1: Abstract\\n============================================================\\nDescription:\\nAn overview of the paper's focus on self-recognition and self-preference in LLM evaluation, summarizing methodology and key findings.\\nRequires Research: \\nTrue\\n\\nContent:\\n## Abstract\\n\\nThis paper examines a unique phenomenon where LLM evaluators exhibit self-recognition and self-preference when judging their own generations. Our work investigates how evaluation systems based on natural language responses may inherently favor outputs originating from the same model. The methodology combines error-based human annotation with meta-evaluation techniques to analyze the extent of evaluation biases. The findings indicate that self-generated outputs are consistently rated higher than independent ones, suggesting a systematic bias in automated evaluation protocols. This study builds on existing frameworks, such as the comprehensive survey of LLMs-as-judges [1] and hierarchical evaluation methods proposed in TencentLLMEval [2]. These sources provide essential insights into current evaluation methodologies and underscore challenges in ensuring objective assessments. Our results advocate for enhanced evaluation protocols that mitigate self-preference, thereby promoting fairness and improving reliability in LLM performance assessments across diverse tasks.\\n\\n### Sources\\n[1] LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods: http://arxiv.org/abs/2412.05579v2  \\n[2] TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for Human-Aligned LLMs: http://arxiv.org/abs/2311.05374v1\\n\\n\\n============================================================\\nSection 2: Introduction\\n============================================================\\nDescription:\\nThis section covers the background, motivation, and the stated research questions regarding how LLM evaluators tend to favor their own outputs.\\nRequires Research: \\nTrue\\n\\nContent:\\n## Introduction\\n\\nLLM evaluators play a crucial role in assessing language generation quality. Recent studies reveal that both user interactions and automated evaluations may be subject to inherent biases. For instance, research indicates users often prefer responses where inaccuracies are unmarked, even when such responses are confidently delivered [1]. This suggests that evaluative systems might similarly be predisposed, potentially leading to an inadvertent favoring of self-generated outputs.\\n\\nMotivated by these observations, recent work has started to explore how fairness in predictive preferences can improve the alignment of automated judgments with human expectations [2]. Such findings prompt questions regarding whether LLM evaluators recognize and favor their own generations, even when alternative responses might be more accurate. Addressing this research question is critical for improving the reliability and trustworthiness of LLM outputs. This investigation underscores the need for methods that mitigate evaluative bias to achieve a more balanced and fair assessment in language technology.\\n\\n### Sources\\n[1] Fool Me, Fool Me: User Attitudes Toward LLM Falsehoods: http://arxiv.org/abs/2412.11625v1  \\n[2] Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments: http://arxiv.org/abs/2406.11370v2\\n\\n\\n============================================================\\nSection 3: Definition and Measurement of Self-Preference and Self-Recognition\\n============================================================\\nDescription:\\nDiscusses the conceptual definitions and detailed experimental designs for measuring self-preference bias and self-recognition capability in LLMs.\\nRequires Research: \\nTrue\\n\\nContent:\\n## Definition and Measurement of Self-Preference and Self-Recognition\\n\\nSelf-preference bias refers to the tendency of a language model to favor its own outputs over those from other sources. Researchers define this bias by comparing evaluation scores assigned to self-generated texts versus externally produced texts. A key aspect of measuring the bias is the use of perplexity, where lower perplexity suggests greater text familiarity and leads to higher self-evaluation [1].\\n\\nExperimental designs for assessing self-recognition involve controlled tests where models evaluate both self-generated and externally sourced responses. The novel quantitative metric examines differences in perplexity and assessment scores, thereby highlighting the role of self-recognition in driving self-preference. In addition, researchers have begun to investigate self-recognition by integrating diverse evaluation frameworks and employing both intrinsic and extrinsic metrics. This multifaceted approach aids in isolating the effect of familiarity and response style on bias, contributing to improved fairness and reliability in automated evaluations.\\n\\n### Sources\\n[1] Self-Preference Bias in LLM-as-a-Judge: http://arxiv.org/abs/2410.21819v1\\n\\n\\n============================================================\\nSection 4: Measuring Correlation between Self-Preference and Self-Recognition\\n============================================================\\nDescription:\\nFocuses on the methodologies and fine-tuning experiments that link self-recognition capability to self-preference bias, including controlled experiments and statistical evaluations.\\nRequires Research: \\nTrue\\n\\nContent:\\n## Measuring Correlation between Self-Preference and Self-Recognition\\n\\nResearch in this area focuses on evaluating how large language models (LLMs) develop self-preference biases through their inherent self-recognition abilities. Panickssery et al. [1] provide experimental evidence that LLMs like GPT-4 and Llama 2 can distinguish their own outputs from those generated by others. Their controlled fine-tuning experiments reveal a linear relationship between the self-recognition score and the strength of self-preference bias, using statistical analyses to rule out straightforward confounders.\\n\\nChen et al. [2] further examine self-preference by applying objective benchmarks in evaluating whether higher scores correspond to genuinely superior outputs or an inherent favoritism. Their methodologies employ verifiable tasks, such as mathematical reasoning and code generation, to differentiate beneficial self-recognition from harmful bias. Collectively, these studies emphasize the importance of controlled evaluation and fine-tuning experiments in understanding the causal link between a models self-recognition capability and its tendency to favor its own generations, contributing to improved reliability and AI safety.\\n\\n### Sources\\n[1] LLM Evaluators Recognize and Favor Their Own Generations: http://arxiv.org/abs/2404.13076v1  \\n[2] Do LLM Evaluators Prefer Themselves for a Reason?: http://arxiv.org/abs/2504.03846v1\\n\\n\\n============================================================\\nSection 5: Related Work\\n============================================================\\nDescription:\\nSummarizes previous literature on LLM self-evaluation, bias in model assessments, and methods in LLM detection and introspection, relating them to the user-provided papers contributions.\\nRequires Research: \\nTrue\\n\\nContent:\\n## Related Work\\nPrevious research has explored whether large language models (LLMs) can reliably introspect their own internal states. For example, Song et al. [1] systematically investigated metalinguistic prompting to compare models self-reported probabilities with their internal string probability estimates, finding limited evidence for privileged self-access. In another vein, studies in robotics have leveraged introspective planning techniques to align LLMs uncertainty with task ambiguity, improving both safety and compliance [2]. Other works introduced introspective tips that allow LLMs to refine decision-making by reflecting on past interactions without parameter fine-tuning [3]. In parallel, bias evaluation in language models has attracted significant attention. Researchers have measured gender bias using both lexicon-based and novel model-based approaches, revealing instability in results dependent on metrics and data dependencies [4]. Additional tools, such as BiasAlert, have been proposed to detect social bias in open-text outputs, providing flexible alternatives to fixed-form assessments [5]. Furthermore, a new benchmark comparing multiple bias evaluation methods in generation tasks highlights inconsistencies between long-form and multiple-choice evaluations [6]. These works lay the groundwork for our study on LLM evaluators favoring their own generations.\\n\\n### Sources\\n[1] Language Models Fail to Introspect About Their Knowledge of Language: http://arxiv.org/abs/2503.07513v2  \\n[2] Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity: http://arxiv.org/abs/2402.06529v4  \\n[3] Introspective Tips: Large Language Model for In-Context Decision Making: http://arxiv.org/abs/2305.11598v1  \\n[4] What is Your Favorite Gender, MLM? Gender Bias Evaluation in Multilingual Masked Language Models: http://arxiv.org/abs/2404.06621v1  \\n[5] BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs: http://arxiv.org/abs/2407.10241v2  \\n[6] Social Bias Benchmark for Generation: A Comparison of Generation and QA-Based Evaluations: http://arxiv.org/abs/2503.06987v1\\n\\n\\n============================================================\\nSection 6: Limitations, Discussion, and Conclusion\\n============================================================\\nDescription:\\nCovers the discussion of safety implications, limitations of the current study, future research directions, and a concluding synthesis of the insights gained.\\nRequires Research: \\nTrue\\n\\nContent:\\n## Limitations, Discussion, and Conclusion\\n\\nThis research section emphasizes several limitations and safety implications of using LLM evaluators. A key finding is the self-preference bias, where models assign higher scores to their own outputs, potentially misrepresenting actual quality and harming fair comparisons [1]. The study also notes inherent limitations in LLM performances, including reasoning flaws, memory issues, and calculation errors that restrict their reliability for critical applications [2].\\n\\nMoreover, our discussion indicates that current evaluation frameworks may fall short in capturing complex real-world interactions. Incorporating human interaction evaluations can strengthen safety assessments and mitigate risks associated with biased self-recognition [3]. In conclusion, while LLM evaluators bring promising advances to automated assessments, their biases and operational limitations call for refined methods and integrated human oversight. Future research should focus on developing robust testing frameworks that blend technical improvements with comprehensive safety and governance strategies.\\n\\n### Sources\\n[1] LLM Evaluators Recognize and Favor Their Own Generations: http://arxiv.org/abs/2404.13076v1  \\n[2] Responsible AI in Construction Safety: Systematic Evaluation of Large Language Models and Prompt Engineering: http://arxiv.org/abs/2411.08320v1  \\n[3] Beyond static AI evaluations: advancing human interaction evaluations for LLM harms and risks: http://arxiv.org/abs/2405.10632v5\\n\\n\", 'sources': [Source(name='LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods', link='http://arxiv.org/abs/2412.05579v2'), Source(name='TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for Human-Aligned LLMs', link='http://arxiv.org/abs/2311.05374v1'), Source(name='Fool Me, Fool Me: User Attitudes Toward LLM Falsehoods', link='http://arxiv.org/abs/2412.11625v1'), Source(name='Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments', link='http://arxiv.org/abs/2406.11370v2'), Source(name='Self-Preference Bias in LLM-as-a-Judge', link='http://arxiv.org/abs/2410.21819v1'), Source(name='LLM Evaluators Recognize and Favor Their Own Generations', link='http://arxiv.org/abs/2404.13076v1'), Source(name='Do LLM Evaluators Prefer Themselves for a Reason?', link='http://arxiv.org/abs/2504.03846v1'), Source(name='Language Models Fail to Introspect About Their Knowledge of Language', link='http://arxiv.org/abs/2503.07513v2'), Source(name=\"Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity\", link='http://arxiv.org/abs/2402.06529v4'), Source(name='Introspective Tips: Large Language Model for In-Context Decision Making', link='http://arxiv.org/abs/2305.11598v1'), Source(name='What is Your Favorite Gender, MLM? Gender Bias Evaluation in Multilingual Masked Language Models', link='http://arxiv.org/abs/2404.06621v1'), Source(name='BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs', link='http://arxiv.org/abs/2407.10241v2'), Source(name='Social Bias Benchmark for Generation: A Comparison of Generation and QA-Based Evaluations', link='http://arxiv.org/abs/2503.06987v1'), Source(name='Responsible AI in Construction Safety: Systematic Evaluation of Large Language Models and Prompt Engineering', link='http://arxiv.org/abs/2411.08320v1'), Source(name='Beyond static AI evaluations: advancing human interaction evaluations for LLM harms and risks', link='http://arxiv.org/abs/2405.10632v5')]}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pass True to approve the report plan \n",
    "async for event in graph.astream(Command(resume=True), thread, stream_mode=\"updates\"):\n",
    "    print(event)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Source(name='LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods', link='http://arxiv.org/abs/2412.05579v2'),\n",
       " Source(name='TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for Human-Aligned LLMs', link='http://arxiv.org/abs/2311.05374v1'),\n",
       " Source(name='Fool Me, Fool Me: User Attitudes Toward LLM Falsehoods', link='http://arxiv.org/abs/2412.11625v1'),\n",
       " Source(name='Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments', link='http://arxiv.org/abs/2406.11370v2'),\n",
       " Source(name='Self-Preference Bias in LLM-as-a-Judge', link='http://arxiv.org/abs/2410.21819v1'),\n",
       " Source(name='LLM Evaluators Recognize and Favor Their Own Generations', link='http://arxiv.org/abs/2404.13076v1'),\n",
       " Source(name='Do LLM Evaluators Prefer Themselves for a Reason?', link='http://arxiv.org/abs/2504.03846v1'),\n",
       " Source(name='Language Models Fail to Introspect About Their Knowledge of Language', link='http://arxiv.org/abs/2503.07513v2'),\n",
       " Source(name=\"Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity\", link='http://arxiv.org/abs/2402.06529v4'),\n",
       " Source(name='Introspective Tips: Large Language Model for In-Context Decision Making', link='http://arxiv.org/abs/2305.11598v1'),\n",
       " Source(name='What is Your Favorite Gender, MLM? Gender Bias Evaluation in Multilingual Masked Language Models', link='http://arxiv.org/abs/2404.06621v1'),\n",
       " Source(name='BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs', link='http://arxiv.org/abs/2407.10241v2'),\n",
       " Source(name='Social Bias Benchmark for Generation: A Comparison of Generation and QA-Based Evaluations', link='http://arxiv.org/abs/2503.06987v1'),\n",
       " Source(name='Responsible AI in Construction Safety: Systematic Evaluation of Large Language Models and Prompt Engineering', link='http://arxiv.org/abs/2411.08320v1'),\n",
       " Source(name='Beyond static AI evaluations: advancing human interaction evaluations for LLM harms and risks', link='http://arxiv.org/abs/2405.10632v5')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state = graph.get_state(thread)\n",
    "sources = final_state.values.get('sources')\n",
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods: http://arxiv.org/abs/2412.05579v2\n",
      "[2] TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for Human-Aligned LLMs: http://arxiv.org/abs/2311.05374v1\n",
      "[3] Fool Me, Fool Me: User Attitudes Toward LLM Falsehoods: http://arxiv.org/abs/2412.11625v1\n",
      "[4] Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments: http://arxiv.org/abs/2406.11370v2\n",
      "[5] Self-Preference Bias in LLM-as-a-Judge: http://arxiv.org/abs/2410.21819v1\n",
      "[6] LLM Evaluators Recognize and Favor Their Own Generations: http://arxiv.org/abs/2404.13076v1\n",
      "[7] Do LLM Evaluators Prefer Themselves for a Reason?: http://arxiv.org/abs/2504.03846v1\n",
      "[8] Language Models Fail to Introspect About Their Knowledge of Language: http://arxiv.org/abs/2503.07513v2\n",
      "[9] Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity: http://arxiv.org/abs/2402.06529v4\n",
      "[10] Introspective Tips: Large Language Model for In-Context Decision Making: http://arxiv.org/abs/2305.11598v1\n",
      "[11] What is Your Favorite Gender, MLM? Gender Bias Evaluation in Multilingual Masked Language Models: http://arxiv.org/abs/2404.06621v1\n",
      "[12] BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs: http://arxiv.org/abs/2407.10241v2\n",
      "[13] Social Bias Benchmark for Generation: A Comparison of Generation and QA-Based Evaluations: http://arxiv.org/abs/2503.06987v1\n",
      "[14] Responsible AI in Construction Safety: Systematic Evaluation of Large Language Models and Prompt Engineering: http://arxiv.org/abs/2411.08320v1\n",
      "[15] Beyond static AI evaluations: advancing human interaction evaluations for LLM harms and risks: http://arxiv.org/abs/2405.10632v5\n"
     ]
    }
   ],
   "source": [
    "# print the source better\n",
    "for i, source in enumerate(sources):\n",
    "    print(f\"[{i+1}] {source.name}: {source.link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "============================================================\n",
       "Section 1: Abstract\n",
       "============================================================\n",
       "Description:\n",
       "An overview of the paper's focus on self-recognition and self-preference in LLM evaluation, summarizing methodology and key findings.\n",
       "Requires Research: \n",
       "True\n",
       "\n",
       "Content:\n",
       "## Abstract\n",
       "\n",
       "This paper examines a unique phenomenon where LLM evaluators exhibit self-recognition and self-preference when judging their own generations. Our work investigates how evaluation systems based on natural language responses may inherently favor outputs originating from the same model. The methodology combines error-based human annotation with meta-evaluation techniques to analyze the extent of evaluation biases. The findings indicate that self-generated outputs are consistently rated higher than independent ones, suggesting a systematic bias in automated evaluation protocols. This study builds on existing frameworks, such as the comprehensive survey of LLMs-as-judges [1] and hierarchical evaluation methods proposed in TencentLLMEval [2]. These sources provide essential insights into current evaluation methodologies and underscore challenges in ensuring objective assessments. Our results advocate for enhanced evaluation protocols that mitigate self-preference, thereby promoting fairness and improving reliability in LLM performance assessments across diverse tasks.\n",
       "\n",
       "### Sources\n",
       "[1] LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods: http://arxiv.org/abs/2412.05579v2  \n",
       "[2] TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for Human-Aligned LLMs: http://arxiv.org/abs/2311.05374v1\n",
       "\n",
       "\n",
       "============================================================\n",
       "Section 2: Introduction\n",
       "============================================================\n",
       "Description:\n",
       "This section covers the background, motivation, and the stated research questions regarding how LLM evaluators tend to favor their own outputs.\n",
       "Requires Research: \n",
       "True\n",
       "\n",
       "Content:\n",
       "## Introduction\n",
       "\n",
       "LLM evaluators play a crucial role in assessing language generation quality. Recent studies reveal that both user interactions and automated evaluations may be subject to inherent biases. For instance, research indicates users often prefer responses where inaccuracies are unmarked, even when such responses are confidently delivered [1]. This suggests that evaluative systems might similarly be predisposed, potentially leading to an inadvertent favoring of self-generated outputs.\n",
       "\n",
       "Motivated by these observations, recent work has started to explore how fairness in predictive preferences can improve the alignment of automated judgments with human expectations [2]. Such findings prompt questions regarding whether LLM evaluators recognize and favor their own generations, even when alternative responses might be more accurate. Addressing this research question is critical for improving the reliability and trustworthiness of LLM outputs. This investigation underscores the need for methods that mitigate evaluative bias to achieve a more balanced and fair assessment in language technology.\n",
       "\n",
       "### Sources\n",
       "[1] Fool Me, Fool Me: User Attitudes Toward LLM Falsehoods: http://arxiv.org/abs/2412.11625v1  \n",
       "[2] Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments: http://arxiv.org/abs/2406.11370v2\n",
       "\n",
       "\n",
       "============================================================\n",
       "Section 3: Definition and Measurement of Self-Preference and Self-Recognition\n",
       "============================================================\n",
       "Description:\n",
       "Discusses the conceptual definitions and detailed experimental designs for measuring self-preference bias and self-recognition capability in LLMs.\n",
       "Requires Research: \n",
       "True\n",
       "\n",
       "Content:\n",
       "## Definition and Measurement of Self-Preference and Self-Recognition\n",
       "\n",
       "Self-preference bias refers to the tendency of a language model to favor its own outputs over those from other sources. Researchers define this bias by comparing evaluation scores assigned to self-generated texts versus externally produced texts. A key aspect of measuring the bias is the use of perplexity, where lower perplexity suggests greater text familiarity and leads to higher self-evaluation [1].\n",
       "\n",
       "Experimental designs for assessing self-recognition involve controlled tests where models evaluate both self-generated and externally sourced responses. The novel quantitative metric examines differences in perplexity and assessment scores, thereby highlighting the role of self-recognition in driving self-preference. In addition, researchers have begun to investigate self-recognition by integrating diverse evaluation frameworks and employing both intrinsic and extrinsic metrics. This multifaceted approach aids in isolating the effect of familiarity and response style on bias, contributing to improved fairness and reliability in automated evaluations.\n",
       "\n",
       "### Sources\n",
       "[1] Self-Preference Bias in LLM-as-a-Judge: http://arxiv.org/abs/2410.21819v1\n",
       "\n",
       "\n",
       "============================================================\n",
       "Section 4: Measuring Correlation between Self-Preference and Self-Recognition\n",
       "============================================================\n",
       "Description:\n",
       "Focuses on the methodologies and fine-tuning experiments that link self-recognition capability to self-preference bias, including controlled experiments and statistical evaluations.\n",
       "Requires Research: \n",
       "True\n",
       "\n",
       "Content:\n",
       "## Measuring Correlation between Self-Preference and Self-Recognition\n",
       "\n",
       "Research in this area focuses on evaluating how large language models (LLMs) develop self-preference biases through their inherent self-recognition abilities. Panickssery et al. [1] provide experimental evidence that LLMs like GPT-4 and Llama 2 can distinguish their own outputs from those generated by others. Their controlled fine-tuning experiments reveal a linear relationship between the self-recognition score and the strength of self-preference bias, using statistical analyses to rule out straightforward confounders.\n",
       "\n",
       "Chen et al. [2] further examine self-preference by applying objective benchmarks in evaluating whether higher scores correspond to genuinely superior outputs or an inherent favoritism. Their methodologies employ verifiable tasks, such as mathematical reasoning and code generation, to differentiate beneficial self-recognition from harmful bias. Collectively, these studies emphasize the importance of controlled evaluation and fine-tuning experiments in understanding the causal link between a models self-recognition capability and its tendency to favor its own generations, contributing to improved reliability and AI safety.\n",
       "\n",
       "### Sources\n",
       "[1] LLM Evaluators Recognize and Favor Their Own Generations: http://arxiv.org/abs/2404.13076v1  \n",
       "[2] Do LLM Evaluators Prefer Themselves for a Reason?: http://arxiv.org/abs/2504.03846v1\n",
       "\n",
       "\n",
       "============================================================\n",
       "Section 5: Related Work\n",
       "============================================================\n",
       "Description:\n",
       "Summarizes previous literature on LLM self-evaluation, bias in model assessments, and methods in LLM detection and introspection, relating them to the user-provided papers contributions.\n",
       "Requires Research: \n",
       "True\n",
       "\n",
       "Content:\n",
       "## Related Work\n",
       "Previous research has explored whether large language models (LLMs) can reliably introspect their own internal states. For example, Song et al. [1] systematically investigated metalinguistic prompting to compare models self-reported probabilities with their internal string probability estimates, finding limited evidence for privileged self-access. In another vein, studies in robotics have leveraged introspective planning techniques to align LLMs uncertainty with task ambiguity, improving both safety and compliance [2]. Other works introduced introspective tips that allow LLMs to refine decision-making by reflecting on past interactions without parameter fine-tuning [3]. In parallel, bias evaluation in language models has attracted significant attention. Researchers have measured gender bias using both lexicon-based and novel model-based approaches, revealing instability in results dependent on metrics and data dependencies [4]. Additional tools, such as BiasAlert, have been proposed to detect social bias in open-text outputs, providing flexible alternatives to fixed-form assessments [5]. Furthermore, a new benchmark comparing multiple bias evaluation methods in generation tasks highlights inconsistencies between long-form and multiple-choice evaluations [6]. These works lay the groundwork for our study on LLM evaluators favoring their own generations.\n",
       "\n",
       "### Sources\n",
       "[1] Language Models Fail to Introspect About Their Knowledge of Language: http://arxiv.org/abs/2503.07513v2  \n",
       "[2] Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity: http://arxiv.org/abs/2402.06529v4  \n",
       "[3] Introspective Tips: Large Language Model for In-Context Decision Making: http://arxiv.org/abs/2305.11598v1  \n",
       "[4] What is Your Favorite Gender, MLM? Gender Bias Evaluation in Multilingual Masked Language Models: http://arxiv.org/abs/2404.06621v1  \n",
       "[5] BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs: http://arxiv.org/abs/2407.10241v2  \n",
       "[6] Social Bias Benchmark for Generation: A Comparison of Generation and QA-Based Evaluations: http://arxiv.org/abs/2503.06987v1\n",
       "\n",
       "\n",
       "============================================================\n",
       "Section 6: Limitations, Discussion, and Conclusion\n",
       "============================================================\n",
       "Description:\n",
       "Covers the discussion of safety implications, limitations of the current study, future research directions, and a concluding synthesis of the insights gained.\n",
       "Requires Research: \n",
       "True\n",
       "\n",
       "Content:\n",
       "## Limitations, Discussion, and Conclusion\n",
       "\n",
       "This research section emphasizes several limitations and safety implications of using LLM evaluators. A key finding is the self-preference bias, where models assign higher scores to their own outputs, potentially misrepresenting actual quality and harming fair comparisons [1]. The study also notes inherent limitations in LLM performances, including reasoning flaws, memory issues, and calculation errors that restrict their reliability for critical applications [2].\n",
       "\n",
       "Moreover, our discussion indicates that current evaluation frameworks may fall short in capturing complex real-world interactions. Incorporating human interaction evaluations can strengthen safety assessments and mitigate risks associated with biased self-recognition [3]. In conclusion, while LLM evaluators bring promising advances to automated assessments, their biases and operational limitations call for refined methods and integrated human oversight. Future research should focus on developing robust testing frameworks that blend technical improvements with comprehensive safety and governance strategies.\n",
       "\n",
       "### Sources\n",
       "[1] LLM Evaluators Recognize and Favor Their Own Generations: http://arxiv.org/abs/2404.13076v1  \n",
       "[2] Responsible AI in Construction Safety: Systematic Evaluation of Large Language Models and Prompt Engineering: http://arxiv.org/abs/2411.08320v1  \n",
       "[3] Beyond static AI evaluations: advancing human interaction evaluations for LLM harms and risks: http://arxiv.org/abs/2405.10632v5\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state = graph.get_state(thread)\n",
    "report = final_state.values.get('report_sections_from_research')\n",
    "Markdown(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opendr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
